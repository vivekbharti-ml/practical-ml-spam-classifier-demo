{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281c892e-a4ca-4a39-9e48-f4e6a659e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a3eac",
   "metadata": {},
   "source": [
    "### Downloading the SMS Spam Collection dataset from the UCI Machine Learning Repository.\n",
    "### This dataset contains labeled SMS messages (spam or ham) and is commonly used for binary text classification tasks.\n",
    "#### Source: https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2ea1b9-b082-4953-9833-38851bf119f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "DATA_DIR = \"../data/raw\"\n",
    "ZIP_PATH = \"smsspamcollection.zip\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, \"SMSSpamCollection\")):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(DATA_URL, ZIP_PATH)\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "    os.remove(ZIP_PATH)\n",
    "    print(\"Dataset downloaded and extracted.\")\n",
    "else:\n",
    "    print(\"Dataset already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea213e9e-2657-4b27-9463-af19b1af201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Spam messages: 747\n",
      "Ham messages: 4825\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = os.path.join(DATA_DIR, \"SMSSpamCollection\")\n",
    "\n",
    "df = pd.read_csv(data_path, sep='\\t', header=None, names=['label', 'text'])\n",
    "print(df.head())\n",
    "print(f\"Spam messages: {df.label.value_counts()['spam']}\")\n",
    "print(f\"Ham messages: {df.label.value_counts()['ham']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d2404cf-4453-4e4c-8286-488cae99206e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce93111",
   "metadata": {},
   "source": [
    "### Creating a holdout test set for final evaluation after all model training and tuning is complete.\n",
    "### This helps simulate how the model will perform on truly unseen data in production.\n",
    "### The holdout set should not be touched during training, validation, or hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b82da8-53cf-4eff-a953-63d2e1aecd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split off 10% holdout set\n",
    "df_train_val, df_holdout = train_test_split(df, test_size=0.1, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Save holdout set separately\n",
    "df_holdout.to_csv('../data/raw/spam_holdout.csv', index=False)\n",
    "\n",
    "# Save training+validation set separately\n",
    "df_train_val.to_csv('../data/raw/spam_train_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183e0607-36ef-4a32-be16-37429d98043f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>ham</td>\n",
       "      <td>Heehee that was so funny tho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>ham</td>\n",
       "      <td>I don wake since. I checked that stuff and saw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dai what this da.. Can i send my resume to thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>ham</td>\n",
       "      <td>U too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ham</td>\n",
       "      <td>Didn't you get hep b immunisation in nigeria.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>ham</td>\n",
       "      <td>What pa tell me.. I went to bath:-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>ham</td>\n",
       "      <td>Jus finish watching tv... U?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>ham</td>\n",
       "      <td>Me fine..absolutly fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>spam</td>\n",
       "      <td>GENT! We are trying to contact you. Last weeke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>ham</td>\n",
       "      <td>We are at grandmas. Oh dear, u still ill? I fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5014 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "3398   ham                       Heehee that was so funny tho\n",
       "3325   ham  I don wake since. I checked that stuff and saw...\n",
       "2498   ham  Dai what this da.. Can i send my resume to thi...\n",
       "1553   ham                                           U too...\n",
       "46     ham      Didn't you get hep b immunisation in nigeria.\n",
       "...    ...                                                ...\n",
       "1932   ham                What pa tell me.. I went to bath:-)\n",
       "5316   ham                       Jus finish watching tv... U?\n",
       "5203   ham                            Me fine..absolutly fine\n",
       "564   spam  GENT! We are trying to contact you. Last weeke...\n",
       "762    ham  We are at grandmas. Oh dear, u still ill? I fe...\n",
       "\n",
       "[5014 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49879c8-dc18-4e44-afbf-9d739a22686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                               text\n",
      "3398   ham                       Heehee that was so funny tho\n",
      "3325   ham  I don wake since. I checked that stuff and saw...\n",
      "2498   ham  Dai what this da.. Can i send my resume to thi...\n",
      "1553   ham                                           U too...\n",
      "46     ham      Didn't you get hep b immunisation in nigeria.\n",
      "Spam messages: 672\n",
      "Ham messages: 4342\n"
     ]
    }
   ],
   "source": [
    "# checking the distribution of training set\n",
    "print(df_train_val.head())\n",
    "print(f\"Spam messages: {df_train_val.label.value_counts()['spam']}\")\n",
    "print(f\"Ham messages: {df_train_val.label.value_counts()['ham']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5d7c70-8ba2-4309-94cf-e43e5945213e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>ham</td>\n",
       "      <td>Or better still can you catch her and let ask ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>spam</td>\n",
       "      <td>Loan for any purpose £500 - £75,000. Homeowner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>ham</td>\n",
       "      <td>Every day i use to sleep after  &amp;lt;#&amp;gt;  so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>ham</td>\n",
       "      <td>Unless it's a situation where YOU GO GURL woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>spam</td>\n",
       "      <td>No. 1 Nokia Tone 4 ur mob every week! Just txt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>spam</td>\n",
       "      <td>+123 Congratulations - in this week's competit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>ham</td>\n",
       "      <td>I sent lanre fakeye's Eckankar details to the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sir send to group mail check it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>ham</td>\n",
       "      <td>If you r @ home then come down within 5 min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>ham</td>\n",
       "      <td>I sent you the prices and do you mean the  &amp;lt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "966    ham  Or better still can you catch her and let ask ...\n",
       "3009  spam  Loan for any purpose £500 - £75,000. Homeowner...\n",
       "2240   ham  Every day i use to sleep after  &lt;#&gt;  so ...\n",
       "297    ham  Unless it's a situation where YOU GO GURL woul...\n",
       "1221  spam  No. 1 Nokia Tone 4 ur mob every week! Just txt...\n",
       "...    ...                                                ...\n",
       "505   spam  +123 Congratulations - in this week's competit...\n",
       "3961   ham  I sent lanre fakeye's Eckankar details to the ...\n",
       "2961   ham                   Sir send to group mail check it.\n",
       "2899   ham        If you r @ home then come down within 5 min\n",
       "2739   ham  I sent you the prices and do you mean the  &lt...\n",
       "\n",
       "[558 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "477e2628-4aec-4a91-b66b-e9c539d47140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                               text\n",
      "966    ham  Or better still can you catch her and let ask ...\n",
      "3009  spam  Loan for any purpose £500 - £75,000. Homeowner...\n",
      "2240   ham  Every day i use to sleep after  &lt;#&gt;  so ...\n",
      "297    ham  Unless it's a situation where YOU GO GURL woul...\n",
      "1221  spam  No. 1 Nokia Tone 4 ur mob every week! Just txt...\n",
      "Spam messages: 75\n",
      "Ham messages: 483\n"
     ]
    }
   ],
   "source": [
    "# checking the distribution of the holdout set\n",
    "print(df_holdout.head())\n",
    "print(f\"Spam messages: {df_holdout.label.value_counts()['spam']}\")\n",
    "print(f\"Ham messages: {df_holdout.label.value_counts()['ham']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0010f",
   "metadata": {},
   "source": [
    "### Preprocessing the training dataset for text classification.\n",
    "### This includes steps like lowercasing, removing punctuation, tokenization, and vectorization.\n",
    "### The goal is to convert raw SMS text messages into numerical feature vectors that can be used by ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f00aaff-8df2-4b68-9728-f177e140d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vivekbharti/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vivekbharti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data files if not already downloaded\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "df_train_val = pd.read_csv('../data/raw/spam_train_val.csv')\n",
    "\n",
    "# Convert labels to binary\n",
    "df_train_val['label_num'] = df_train_val.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    # Re-join tokens to return a clean string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train_val['clean_text'] = df_train_val['text'].apply(preprocess_text)\n",
    "\n",
    "# Prepare final data\n",
    "X = df_train_val['clean_text']\n",
    "y = df_train_val['label_num']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e63fd9e-d5b1-4bf3-afe9-f5e992620e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       869\n",
      "           1       0.97      0.66      0.79       134\n",
      "\n",
      "    accuracy                           0.95      1003\n",
      "   macro avg       0.96      0.83      0.88      1003\n",
      "weighted avg       0.95      0.95      0.95      1003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eb33d",
   "metadata": {},
   "source": [
    "### Training and evaluating multiple models to compare performance and select the best one based on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab52137-0ce8-4a54-a948-5ec027fe46da",
   "metadata": {},
   "source": [
    "#### Choosing the best model isn’t always black and white — while SVM showed marginally better overall accuracy, Logistic Regression had slightly higher recall with comparable performance. Since recall is often critical in spam detection tasks, I leaned toward Logistic Regression — but both are valid choices depending on the priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3001c735-44a5-4a08-bf97-8c245ba12d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Accuracy: 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       869\n",
      "           1       0.92      0.92      0.92       134\n",
      "\n",
      "    accuracy                           0.98      1003\n",
      "   macro avg       0.96      0.95      0.95      1003\n",
      "weighted avg       0.98      0.98      0.98      1003\n",
      "\n",
      "----------------------------------------\n",
      "Model: SVM\n",
      "Accuracy: 0.9831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       869\n",
      "           1       0.97      0.90      0.93       134\n",
      "\n",
      "    accuracy                           0.98      1003\n",
      "   macro avg       0.98      0.95      0.96      1003\n",
      "weighted avg       0.98      0.98      0.98      1003\n",
      "\n",
      "----------------------------------------\n",
      "Model: MultinomialNB\n",
      "Accuracy: 0.9571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       869\n",
      "           1       1.00      0.68      0.81       134\n",
      "\n",
      "    accuracy                           0.96      1003\n",
      "   macro avg       0.98      0.84      0.89      1003\n",
      "weighted avg       0.96      0.96      0.95      1003\n",
      "\n",
      "----------------------------------------\n",
      "Model: RandomForest\n",
      "Accuracy: 0.9621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       869\n",
      "           1       0.99      0.72      0.84       134\n",
      "\n",
      "    accuracy                           0.96      1003\n",
      "   macro avg       0.97      0.86      0.91      1003\n",
      "weighted avg       0.96      0.96      0.96      1003\n",
      "\n",
      "----------------------------------------\n",
      "Model with best accuracy: SVM with accuracy 0.9831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Models with class_weight where applicable\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', class_weight='balanced', probability=True, random_state=42),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "}\n",
    "\n",
    "best_model_name = None\n",
    "best_accuracy = 0\n",
    "best_pipeline = None\n",
    "\n",
    "for name, clf in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ('clf', clf),\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('-' * 40)\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_model_name = name\n",
    "        best_pipeline = pipeline\n",
    "\n",
    "print(f\"Model with best accuracy: {best_model_name} with accuracy {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb92897",
   "metadata": {},
   "source": [
    "### Tuning hyperparameters for Logistic Regression to find the best-performing configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806da695-0282-4f22-ba3a-a9dc826edecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................clf__C=0.01, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ................clf__C=0.01, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ............clf__C=0.01, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ............clf__C=0.01, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ................clf__C=0.01, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ................clf__C=0.01, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ................clf__C=0.01, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ............clf__C=0.01, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ............clf__C=0.01, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ............clf__C=0.01, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .................clf__C=0.1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .................clf__C=0.1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .................clf__C=0.1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .............clf__C=0.1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .................clf__C=0.1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .............clf__C=0.1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .................clf__C=0.1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .............clf__C=0.1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .............clf__C=0.1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ...................clf__C=1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .............clf__C=0.1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ...................clf__C=1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ...................clf__C=1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ...................clf__C=1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ...................clf__C=1, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ...............clf__C=1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ...............clf__C=1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ...............clf__C=1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ...............clf__C=1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ..................clf__C=10, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ...............clf__C=1, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ..................clf__C=10, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ..................clf__C=10, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ..................clf__C=10, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ..................clf__C=10, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END ..............clf__C=10, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ..............clf__C=10, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ..............clf__C=10, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ..............clf__C=10, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END ..............clf__C=10, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .................clf__C=100, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .................clf__C=100, clf__class_weight=None; total time=   0.1s\n",
      "[CV] END .................clf__C=100, clf__class_weight=None; total time=   0.0s\n",
      "[CV] END .................clf__C=100, clf__class_weight=None; total time=   0.1s\n",
      "[CV] END .............clf__C=100, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .................clf__C=100, clf__class_weight=None; total time=   0.1s\n",
      "[CV] END .............clf__C=100, clf__class_weight=balanced; total time=   0.1s\n",
      "[CV] END .............clf__C=100, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .............clf__C=100, clf__class_weight=balanced; total time=   0.0s\n",
      "[CV] END .............clf__C=100, clf__class_weight=balanced; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(max_iter=1000,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;clf__class_weight&#x27;: [None, &#x27;balanced&#x27;]},\n",
       "             scoring=make_scorer(recall_score, pos_label=1), verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(max_iter=1000,\n",
       "                                                           solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__C&#x27;: [0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;clf__class_weight&#x27;: [None, &#x27;balanced&#x27;]},\n",
       "             scoring=make_scorer(recall_score, pos_label=1), verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(max_iter=1000,\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'clf__class_weight': [None, 'balanced']},\n",
       "             scoring=make_scorer(recall_score, pos_label=1), verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### optional: in this example it didn't help\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression(max_iter=1000, solver='liblinear')),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'clf__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Use recall on positive class (spam) as scoring metric\n",
    "recall_scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring=recall_scorer, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc231955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 10, 'clf__class_weight': 'balanced'}\n",
      "Best recall: 0.9088785046728972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       869\n",
      "           1       0.97      0.91      0.94       134\n",
      "\n",
      "    accuracy                           0.98      1003\n",
      "   macro avg       0.98      0.95      0.96      1003\n",
      "weighted avg       0.98      0.98      0.98      1003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best recall:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5b2cc",
   "metadata": {},
   "source": [
    "### Training the final model using the entire dataset (train + test) with default parameters\n",
    "### We proceed with default parameters since hyperparameter tuning did not yield significant improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3b8c54e-2bf3-4e72-a390-5c5165443971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000,\n",
       "                                    random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;clf&#x27;,\n",
       "                 LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000,\n",
       "                                    random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(stop_words='english')),\n",
       "                ('clf',\n",
       "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
       "                                    random_state=42))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Merge train and test sets\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "# Define and train pipeline\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)),\n",
    "])\n",
    "\n",
    "logreg_pipeline.fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed59380",
   "metadata": {},
   "source": [
    "### Saving the final Logistic Regression model as a pickle file for later use in model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f33b9f19-608f-42a8-bd05-c4c57d6ebf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logistic Regression model and TF-IDF vectorizer saved to logreg_spam_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../models/logreg_spam_pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(logreg_pipeline, f)\n",
    "\n",
    "print(\"✅ Logistic Regression model and TF-IDF vectorizer saved to logreg_spam_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a4e92",
   "metadata": {},
   "source": [
    "### Getting the best theshold cut for the model based on precision recall tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d756240-a94e-4559-b5b6-b9014a94eae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds with precision ≥ 0.9:\n",
      "Threshold: 0.342, Precision: 0.905, Recall: 1.000\n",
      "Threshold: 0.355, Precision: 0.912, Recall: 1.000\n",
      "Threshold: 0.402, Precision: 0.918, Recall: 1.000\n",
      "Threshold: 0.440, Precision: 0.924, Recall: 1.000\n",
      "Threshold: 0.463, Precision: 0.931, Recall: 1.000\n",
      "Threshold: 0.464, Precision: 0.944, Recall: 1.000\n",
      "Threshold: 0.479, Precision: 0.950, Recall: 1.000\n",
      "Threshold: 0.480, Precision: 0.957, Recall: 1.000\n",
      "Threshold: 0.496, Precision: 0.964, Recall: 1.000\n",
      "Threshold: 0.511, Precision: 0.971, Recall: 1.000\n",
      "Threshold: 0.533, Precision: 0.971, Recall: 0.993\n",
      "Threshold: 0.537, Precision: 0.978, Recall: 0.993\n",
      "Threshold: 0.590, Precision: 0.978, Recall: 0.985\n",
      "Threshold: 0.595, Precision: 0.978, Recall: 0.978\n",
      "Threshold: 0.620, Precision: 0.985, Recall: 0.978\n",
      "Threshold: 0.621, Precision: 0.985, Recall: 0.970\n",
      "Threshold: 0.626, Precision: 0.985, Recall: 0.963\n",
      "Threshold: 0.640, Precision: 0.985, Recall: 0.955\n",
      "Threshold: 0.644, Precision: 0.984, Recall: 0.948\n",
      "Threshold: 0.655, Precision: 0.984, Recall: 0.940\n",
      "Threshold: 0.656, Precision: 0.984, Recall: 0.933\n",
      "Threshold: 0.680, Precision: 0.984, Recall: 0.925\n",
      "Threshold: 0.688, Precision: 0.992, Recall: 0.925\n",
      "Threshold: 0.696, Precision: 0.992, Recall: 0.918\n",
      "Threshold: 0.699, Precision: 0.992, Recall: 0.910\n",
      "Threshold: 0.710, Precision: 0.992, Recall: 0.903\n",
      "Threshold: 0.722, Precision: 0.992, Recall: 0.896\n",
      "Threshold: 0.724, Precision: 0.992, Recall: 0.888\n",
      "Threshold: 0.730, Precision: 0.992, Recall: 0.873\n",
      "Threshold: 0.733, Precision: 1.000, Recall: 0.873\n",
      "Threshold: 0.740, Precision: 1.000, Recall: 0.866\n",
      "Threshold: 0.742, Precision: 1.000, Recall: 0.858\n",
      "Threshold: 0.751, Precision: 1.000, Recall: 0.851\n",
      "Threshold: 0.764, Precision: 1.000, Recall: 0.843\n",
      "Threshold: 0.767, Precision: 1.000, Recall: 0.836\n",
      "Threshold: 0.773, Precision: 1.000, Recall: 0.828\n",
      "Threshold: 0.778, Precision: 1.000, Recall: 0.821\n",
      "Threshold: 0.779, Precision: 1.000, Recall: 0.813\n",
      "Threshold: 0.787, Precision: 1.000, Recall: 0.806\n",
      "Threshold: 0.791, Precision: 1.000, Recall: 0.799\n",
      "Threshold: 0.791, Precision: 1.000, Recall: 0.791\n",
      "Threshold: 0.798, Precision: 1.000, Recall: 0.784\n",
      "Threshold: 0.801, Precision: 1.000, Recall: 0.776\n",
      "Threshold: 0.802, Precision: 1.000, Recall: 0.769\n",
      "Threshold: 0.813, Precision: 1.000, Recall: 0.761\n",
      "Threshold: 0.815, Precision: 1.000, Recall: 0.754\n",
      "Threshold: 0.821, Precision: 1.000, Recall: 0.746\n",
      "Threshold: 0.823, Precision: 1.000, Recall: 0.739\n",
      "Threshold: 0.827, Precision: 1.000, Recall: 0.731\n",
      "Threshold: 0.829, Precision: 1.000, Recall: 0.724\n",
      "Threshold: 0.833, Precision: 1.000, Recall: 0.716\n",
      "Threshold: 0.833, Precision: 1.000, Recall: 0.709\n",
      "Threshold: 0.834, Precision: 1.000, Recall: 0.694\n",
      "Threshold: 0.845, Precision: 1.000, Recall: 0.687\n",
      "Threshold: 0.851, Precision: 1.000, Recall: 0.679\n",
      "Threshold: 0.854, Precision: 1.000, Recall: 0.672\n",
      "Threshold: 0.860, Precision: 1.000, Recall: 0.664\n",
      "Threshold: 0.861, Precision: 1.000, Recall: 0.657\n",
      "Threshold: 0.862, Precision: 1.000, Recall: 0.649\n",
      "Threshold: 0.865, Precision: 1.000, Recall: 0.642\n",
      "Threshold: 0.869, Precision: 1.000, Recall: 0.634\n",
      "Threshold: 0.872, Precision: 1.000, Recall: 0.627\n",
      "Threshold: 0.876, Precision: 1.000, Recall: 0.619\n",
      "Threshold: 0.877, Precision: 1.000, Recall: 0.612\n",
      "Threshold: 0.883, Precision: 1.000, Recall: 0.604\n",
      "Threshold: 0.884, Precision: 1.000, Recall: 0.597\n",
      "Threshold: 0.891, Precision: 1.000, Recall: 0.575\n",
      "Threshold: 0.897, Precision: 1.000, Recall: 0.567\n",
      "Threshold: 0.903, Precision: 1.000, Recall: 0.560\n",
      "Threshold: 0.904, Precision: 1.000, Recall: 0.552\n",
      "Threshold: 0.907, Precision: 1.000, Recall: 0.545\n",
      "Threshold: 0.911, Precision: 1.000, Recall: 0.537\n",
      "Threshold: 0.911, Precision: 1.000, Recall: 0.530\n",
      "Threshold: 0.913, Precision: 1.000, Recall: 0.522\n",
      "Threshold: 0.914, Precision: 1.000, Recall: 0.515\n",
      "Threshold: 0.916, Precision: 1.000, Recall: 0.507\n",
      "Threshold: 0.918, Precision: 1.000, Recall: 0.500\n",
      "Threshold: 0.918, Precision: 1.000, Recall: 0.493\n",
      "Threshold: 0.919, Precision: 1.000, Recall: 0.485\n",
      "Threshold: 0.925, Precision: 1.000, Recall: 0.478\n",
      "Threshold: 0.925, Precision: 1.000, Recall: 0.470\n",
      "Threshold: 0.925, Precision: 1.000, Recall: 0.463\n",
      "Threshold: 0.928, Precision: 1.000, Recall: 0.455\n",
      "Threshold: 0.929, Precision: 1.000, Recall: 0.448\n",
      "Threshold: 0.930, Precision: 1.000, Recall: 0.440\n",
      "Threshold: 0.931, Precision: 1.000, Recall: 0.433\n",
      "Threshold: 0.932, Precision: 1.000, Recall: 0.425\n",
      "Threshold: 0.933, Precision: 1.000, Recall: 0.418\n",
      "Threshold: 0.933, Precision: 1.000, Recall: 0.410\n",
      "Threshold: 0.934, Precision: 1.000, Recall: 0.403\n",
      "Threshold: 0.935, Precision: 1.000, Recall: 0.396\n",
      "Threshold: 0.936, Precision: 1.000, Recall: 0.388\n",
      "Threshold: 0.936, Precision: 1.000, Recall: 0.381\n",
      "Threshold: 0.936, Precision: 1.000, Recall: 0.373\n",
      "Threshold: 0.936, Precision: 1.000, Recall: 0.366\n",
      "Threshold: 0.937, Precision: 1.000, Recall: 0.358\n",
      "Threshold: 0.938, Precision: 1.000, Recall: 0.351\n",
      "Threshold: 0.938, Precision: 1.000, Recall: 0.343\n",
      "Threshold: 0.940, Precision: 1.000, Recall: 0.336\n",
      "Threshold: 0.941, Precision: 1.000, Recall: 0.313\n",
      "Threshold: 0.941, Precision: 1.000, Recall: 0.306\n",
      "Threshold: 0.948, Precision: 1.000, Recall: 0.299\n",
      "Threshold: 0.950, Precision: 1.000, Recall: 0.291\n",
      "Threshold: 0.951, Precision: 1.000, Recall: 0.284\n",
      "Threshold: 0.953, Precision: 1.000, Recall: 0.276\n",
      "Threshold: 0.953, Precision: 1.000, Recall: 0.269\n",
      "Threshold: 0.961, Precision: 1.000, Recall: 0.261\n",
      "Threshold: 0.961, Precision: 1.000, Recall: 0.254\n",
      "Threshold: 0.961, Precision: 1.000, Recall: 0.239\n",
      "Threshold: 0.961, Precision: 1.000, Recall: 0.231\n",
      "Threshold: 0.962, Precision: 1.000, Recall: 0.224\n",
      "Threshold: 0.962, Precision: 1.000, Recall: 0.216\n",
      "Threshold: 0.963, Precision: 1.000, Recall: 0.209\n",
      "Threshold: 0.963, Precision: 1.000, Recall: 0.201\n",
      "Threshold: 0.964, Precision: 1.000, Recall: 0.194\n",
      "Threshold: 0.964, Precision: 1.000, Recall: 0.187\n",
      "Threshold: 0.964, Precision: 1.000, Recall: 0.179\n",
      "Threshold: 0.965, Precision: 1.000, Recall: 0.172\n",
      "Threshold: 0.966, Precision: 1.000, Recall: 0.164\n",
      "Threshold: 0.966, Precision: 1.000, Recall: 0.157\n",
      "Threshold: 0.969, Precision: 1.000, Recall: 0.149\n",
      "Threshold: 0.970, Precision: 1.000, Recall: 0.142\n",
      "Threshold: 0.970, Precision: 1.000, Recall: 0.134\n",
      "Threshold: 0.971, Precision: 1.000, Recall: 0.127\n",
      "Threshold: 0.971, Precision: 1.000, Recall: 0.119\n",
      "Threshold: 0.972, Precision: 1.000, Recall: 0.112\n",
      "Threshold: 0.973, Precision: 1.000, Recall: 0.104\n",
      "Threshold: 0.974, Precision: 1.000, Recall: 0.097\n",
      "Threshold: 0.976, Precision: 1.000, Recall: 0.090\n",
      "Threshold: 0.979, Precision: 1.000, Recall: 0.082\n",
      "Threshold: 0.981, Precision: 1.000, Recall: 0.075\n",
      "Threshold: 0.983, Precision: 1.000, Recall: 0.067\n",
      "Threshold: 0.985, Precision: 1.000, Recall: 0.060\n",
      "Threshold: 0.988, Precision: 1.000, Recall: 0.052\n",
      "Threshold: 0.990, Precision: 1.000, Recall: 0.045\n",
      "Threshold: 0.990, Precision: 1.000, Recall: 0.037\n",
      "Threshold: 0.990, Precision: 1.000, Recall: 0.030\n",
      "Threshold: 0.991, Precision: 1.000, Recall: 0.022\n",
      "Threshold: 0.991, Precision: 1.000, Recall: 0.015\n",
      "Threshold: 0.991, Precision: 1.000, Recall: 0.007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get predicted probabilities from the logistic regression pipeline\n",
    "y_probs = logreg_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "# Print thresholds where precision >= 0.9\n",
    "print(\"Thresholds with precision ≥ 0.9:\")\n",
    "for p, r, t in zip(precision, recall, thresholds):\n",
    "    if p >= 0.9:\n",
    "        print(f\"Threshold: {t:.3f}, Precision: {p:.3f}, Recall: {r:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33532ea8-b00b-4d24-88fb-81d42ce4cb66",
   "metadata": {},
   "source": [
    "best thresh: best_threshold = 0.619"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9e7a4-20b2-4d19-aaf9-7bc082226b59",
   "metadata": {},
   "source": [
    "### Testing final model performance on holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66c86009-941a-43b7-92f2-c93ff070f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vivekbharti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vivekbharti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.98      0.98       483\n",
      "        spam       0.88      0.89      0.89        75\n",
      "\n",
      "    accuracy                           0.97       558\n",
      "   macro avg       0.93      0.94      0.93       558\n",
      "weighted avg       0.97      0.97      0.97       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# === Load saved pipeline ===\n",
    "with open('../models/logreg_spam_pipeline.pkl', 'rb') as f:\n",
    "    logreg_pipeline = pickle.load(f)\n",
    "\n",
    "# === Load holdout dataset ===\n",
    "df_holdout = pd.read_csv('../data/raw/spam_holdout.csv')\n",
    "df_holdout['label_num'] = df_holdout.label.map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# === Preprocessing function ===\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# === Apply preprocessing ===\n",
    "df_holdout['clean_text'] = df_holdout['text'].apply(preprocess_text)\n",
    "\n",
    "# === Predict using loaded pipeline ===\n",
    "X_holdout = df_holdout['clean_text']\n",
    "y_holdout = df_holdout['label_num']\n",
    "y_pred = logreg_pipeline.predict(X_holdout)\n",
    "\n",
    "# === Classification report ===\n",
    "print(classification_report(y_holdout, y_pred, target_names=['ham', 'spam']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-classifier-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
